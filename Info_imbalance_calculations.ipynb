{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da59506d",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771ac65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV,ShuffleSplit\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ffc88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Normal_MC.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8467c7f",
   "metadata": {},
   "source": [
    "##### Making Features and Target Matrices\n",
    "\n",
    "The dataset contaain both structural descripotrs and physicsal descripotors, here we are just going to consider the Physical Descripotrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db7c6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344]\n",
    "\n",
    "XX = df.iloc[:,L] #Has shape 40000*60 (4000* 10 configurations)\n",
    "YY = df.iloc[:,3] #Has shape 40000*1  (4000* 10 configurations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d6866b",
   "metadata": {},
   "source": [
    "# Information Imbalance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6173078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_imbalance(distances_A, distances_B, k=1):\n",
    "    N = distances_A.shape[0]\n",
    "    \n",
    "    ranks_A = np.argsort(distances_A, axis=1)\n",
    "    ranks_B = np.argsort(distances_B, axis=1)\n",
    "    \n",
    "\n",
    "    imbalance_A_to_B = compute_imbalance(ranks_A, ranks_B, N, 1)\n",
    "    imbalance_B_to_A = compute_imbalance(ranks_B, ranks_A, N, 1)\n",
    "    \n",
    "    return imbalance_A_to_B, imbalance_B_to_A\n",
    "\n",
    "def compute_imbalance(ranks_from, ranks_to, N = 4000, l = 1): # l here corresponding to k in information imbalance paper, and we are calculating imbakllance for k=1\n",
    "\n",
    "    nn_indices = ranks_from[:, 1:l+1] \n",
    "    \n",
    "\n",
    "    nn_ranks_in_to = np.array([\n",
    "        [np.where(ranks_to[i] == nn_indices[i, j])[0][0] for j in range(l)]\n",
    "        for i in range(N)\n",
    "    ])\n",
    "    \n",
    "    avg_rank = np.mean(nn_ranks_in_to) / N\n",
    "    imbalance = 2 * avg_rank\n",
    "    \n",
    "    return imbalance\n",
    "\n",
    "def compute_symmetric_imbalance(dist_all, selected_descriptors):\n",
    "    combined_data = np.hstack([d1[:,j].reshape(-1, 1) for j in selected_descriptors])\n",
    "    dist_combined = squareform(pdist(combined_data))\n",
    "    imbalance_all_combined = compute_information_imbalance(dist_all, dist_combined)\n",
    "    symmetric_imbalance = (imbalance_all_combined[0] + imbalance_all_combined[1]) / np.sqrt(2)\n",
    "    return symmetric_imbalance , imbalance_all_combined[0] , imbalance_all_combined[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e8e93",
   "metadata": {},
   "source": [
    "## Supervised One\n",
    "\n",
    "comparing descripotrs space with propensity space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecedff",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_cnf = []\n",
    "imbalance_jp_cnf = []\n",
    "imbalance_pj_cnf = []\n",
    "\n",
    "\n",
    "for cnf in range(1,11):\n",
    "    \n",
    "    X =XX.iloc[4000*(cnf-1):4000*cnf]\n",
    "    y = YY.iloc[4000*(cnf-1):4000*cnf]\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    y = np.array(y)  \n",
    "    \n",
    "\n",
    "\n",
    "    dist_p = squareform(pdist(y.reshape(-1,1)))\n",
    "    best_descriptors = []\n",
    "    imbalance_jp = []\n",
    "    imbalance_pj = []\n",
    "    best_j_p = float('inf')\n",
    "\n",
    "    \n",
    "    #Calculating 1st Descriptor\n",
    "    for j in range(0,60):\n",
    "\n",
    "        dist_j = squareform(pdist(X[:, j].reshape(-1, 1)))\n",
    "        imbalance_J_to_P, imbalance_P_to_J = compute_information_imbalance(dist_j , dist_p)\n",
    "\n",
    "        if imbalance_J_to_P < best_j_p : \n",
    "            best_j_p = imbalance_J_to_P\n",
    "            same_p_j = imbalance_P_to_J\n",
    "            best_descriptors = [j]\n",
    "\n",
    "\n",
    "    imbalance_jp.append(best_j_p)\n",
    "    imbalance_pj.append(same_p_j)        \n",
    "    #print(f\"best J_P = {best_j_p}\")\n",
    "    #print(f\"Same P_J = {same_p_j}\")\n",
    "    #print(f\"best_descriptors = {best_descriptors}\")\n",
    "\n",
    "\n",
    "    #Calculating Remaining Descriptors\n",
    "    for _ in range(14):\n",
    "        best_j_p = float('inf') \n",
    "        current_best_descriptor = None \n",
    "\n",
    "        for j in range(0,60):\n",
    "            if j in best_descriptors:\n",
    "                continue\n",
    "\n",
    "            candidate_descriptors = best_descriptors + [j] \n",
    "            combined_data = np.hstack([X[:,j].reshape(-1, 1) for j in candidate_descriptors])\n",
    "\n",
    "            dist_new = squareform(pdist(combined_data))      \n",
    "            imbalance_J_to_P, imbalance_P_to_J = compute_information_imbalance(dist_new, dist_p)\n",
    "\n",
    "            if imbalance_J_to_P < best_j_p: \n",
    "                best_j_p = imbalance_J_to_P\n",
    "                same_p_j = imbalance_P_to_J \n",
    "                current_best_descriptor = j\n",
    "\n",
    "        if current_best_descriptor is not None:\n",
    "            best_descriptors.append(current_best_descriptor)\n",
    "            best_imbalance =  best_j_p\n",
    "            imbalance_jp.append(best_j_p)\n",
    "            imbalance_pj.append(same_p_j)\n",
    "            print(f\"Updated best symmetric_imbalance: {best_imbalance}\")\n",
    "            print(f\"same p_j = {same_p_j}\")\n",
    "            print(f\"Selected descriptors: {best_descriptors}\")\n",
    "    \n",
    "    \n",
    "    # Adding Random Number descriptor\n",
    "    random_descriptor = np.random.rand(X.shape[0], 1)\n",
    "    selected_descriptors = np.hstack([X[:, best_descriptors], random_descriptor]) \n",
    "    dist_combined = squareform(pdist(selected_descriptors))\n",
    "    imbalance_J_to_P, imbalance_P_to_J = compute_information_imbalance(dist_combined, dist_p)\n",
    "    best_descriptors.append(1000)\n",
    "    # index 1000 is random number vector\n",
    "    \n",
    "    imbalance_jp.append(imbalance_J_to_P)\n",
    "    imbalance_pj.append(imbalance_P_to_J)\n",
    "    \n",
    "    print(f\"Imbalance with random descriptor - J to P: {imbalance_J_to_P}\")\n",
    "    print(f\"Imbalance with random descriptor - P to J: {imbalance_P_to_J}\")\n",
    "    print(f\"Final descriptors (including random): {best_descriptors}\")\n",
    "    \n",
    "    descriptors_cnf.append(best_descriptors)\n",
    "    imbalance_jp_cnf.append(imbalance_jp)\n",
    "    imbalance_pj_cnf.append(imbalance_pj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457425ed",
   "metadata": {},
   "source": [
    "Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_jp = np.array(imbalance_jp_cnf)\n",
    "arr_pj = np.array(imbalance_pj_cnf)\n",
    "arr_cnf = np.array(descriptors_cnf)\n",
    "\n",
    "Arr_imb_sup = np.vstack([arr_jp,arr_pj,arr_cnf])\n",
    "\n",
    "#Arr_imb_sup contain first 10 columns as arr_jp, next 10 as arr_pj and last 10 as arr_cnf \n",
    "np.save(\"Innformation_imbalance_supervised\", Arr_imb_sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49749d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load array, use this\n",
    "Imbalance_supervised = np.load(\"Innformation_imbalance_supervised.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d41731",
   "metadata": {},
   "source": [
    "## Unsupervised one\n",
    "\n",
    "Comparing the space of Descripotrs with a smaller subset of fewer descripotrs among them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_descrip_all = []\n",
    "best_imbalance_all = []\n",
    "best_pj_all = []\n",
    "best_jp_all = []\n",
    "\n",
    "for cnf in range(1,11):\n",
    "    \n",
    "    df_new =XX.iloc[4000*(cnf-1):4000*cnf]\n",
    "    scaler = StandardScaler()\n",
    "    d1 = scaler.fit_transform(df_new)\n",
    "    \n",
    "    \n",
    "    # Example usage\n",
    "    np.random.seed(42)\n",
    "    best_descriptors = []\n",
    "    sym_imbalance = []\n",
    "    best_pj = []\n",
    "    best_jp = []\n",
    "    \n",
    "    dist_all = squareform(pdist(d1))\n",
    "    best_symmetric_imbalance = float('inf')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for j in range(0,60):\n",
    "        dist_j = squareform(pdist(d1[:, j].reshape(-1, 1)))\n",
    "        imbalance_all_j = compute_information_imbalance(dist_all, dist_j)\n",
    "        symmetric_imbalance = (imbalance_all_j[0] + imbalance_all_j[1]) / np.sqrt(2)\n",
    "\n",
    "        if symmetric_imbalance < best_symmetric_imbalance:\n",
    "            best_symmetric_imbalance = symmetric_imbalance\n",
    "            best_descriptors = [j]\n",
    "            sym_imbalance = [best_symmetric_imbalance]\n",
    "            best_pj = [imbalance_all_j[0]]\n",
    "            best_jp = [imbalance_all_j[1]]\n",
    "    \n",
    "\n",
    "    for _ in range(1, 15):\n",
    "        current_best_symmetric_imbalance = float('inf')\n",
    "        current_best_descriptor = None\n",
    "\n",
    "        for j in range(0,60):\n",
    "            if j in best_descriptors:\n",
    "                continue\n",
    "\n",
    "            candidate_descriptors = best_descriptors + [j]\n",
    "            symmetric_imbalance , pj , jp = compute_symmetric_imbalance(dist_all, candidate_descriptors)\n",
    "\n",
    "            if symmetric_imbalance < current_best_symmetric_imbalance:\n",
    "                current_best_symmetric_imbalance = symmetric_imbalance\n",
    "                current_best_descriptor = j\n",
    "                current_best_pj = pj\n",
    "                current_best_jp = jp\n",
    "\n",
    "        if current_best_descriptor is not None:\n",
    "            best_descriptors.append(current_best_descriptor)\n",
    "            best_pj.append(current_best_pj)\n",
    "            best_jp.append(current_best_jp)\n",
    "            \n",
    "            sym_imbalance.append(current_best_symmetric_imbalance)\n",
    "            best_symmetric_imbalance = current_best_symmetric_imbalance\n",
    "           \n",
    "    print(f\"Final best symmetric_imbalance: {best_symmetric_imbalance}\")\n",
    "    print(f\"Final selected descriptors: {best_descriptors}\")\n",
    "    #print(best_pj)\n",
    "    #print(best_jp)\n",
    "    \n",
    "    best_pj_all.append(best_pj)\n",
    "    best_jp_all.append(best_jp)\n",
    "    best_descrip_all.append(best_descriptors)\n",
    "    best_imbalance_all.append(sym_imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e826ff1",
   "metadata": {},
   "source": [
    "Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_arr = np.array(best_descrip_all)\n",
    "imbalance_arr = np.array(best_imbalance_all)\n",
    "pj_array = np.array(best_pj_all)\n",
    "jp_array = np.array(best_jp_all)\n",
    "\n",
    "imbalance_unsup = np.vstack([desc_arr,imbalance_arr ,pj_array,jp_array])\n",
    "np.save(\"Innformation_imbalance_unsupervised\", imbalance_unsup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d3a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_unsup = np.load(\"Innformation_imbalance_unsupervised.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf83a8a",
   "metadata": {},
   "source": [
    "## Information_imbalance_each_descriptor_vs_propensity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fad3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_cnf = []\n",
    "imbalance_jp_cnf = []\n",
    "imbalance_pj_cnf = []\n",
    "\n",
    "\n",
    "for cnf in range(1,11):\n",
    "    \n",
    "    X =XX.iloc[4000*(cnf-1):4000*cnf]\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    Y = np.array(YY.iloc[4000*(cnf-1):4000*cnf])\n",
    "    \n",
    "    dist_p = squareform(pdist(Y.reshape(-1,1)))\n",
    "    best_descriptors = []\n",
    "    imbalance_jp = []\n",
    "    imbalance_pj = []\n",
    "    best_j_p = float('inf')\n",
    "\n",
    "    for j in range(0,60):\n",
    "\n",
    "        dist_j = squareform(pdist(X[:, j].reshape(-1, 1)))\n",
    "        imbalance_J_to_P, imbalance_P_to_J = compute_information_imbalance(dist_j , dist_p)\n",
    "        imbalance_jp.append(imbalance_J_to_P)\n",
    "        imbalance_pj.append(imbalance_P_to_J)        \n",
    "        \n",
    "    imbalance_jp_cnf.append(imbalance_jp)\n",
    "    imbalance_pj_cnf.append(imbalance_pj)\n",
    "    print(cnf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8853fb",
   "metadata": {},
   "source": [
    "Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09910eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_jp = np.array(imbalance_jp_cnf)\n",
    "arr_pj = np.array(imbalance_pj_cnf)\n",
    "\n",
    "Arr_des_prop  = np.vstack([arr_jp,arr_pj]) \n",
    "np.save(\"Innformation_imbalance_each_descripotr_vs_propensity\", Arr_des_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f3b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Arr_des_prop = np.load(\"Innformation_imbalance_each_descripotr_vs_propensity.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f731e67",
   "metadata": {},
   "source": [
    "# Regression via various methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e1323",
   "metadata": {},
   "source": [
    "## Lasso Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def find_alpha_for_lasso(X_train, y_train, num_features, tol=1e-4, max_iter=1000):\n",
    "    alpha = 1.0\n",
    "    step = alpha / 2.0\n",
    "    selected_features = 0\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        lasso = Lasso(alpha=alpha, tol=tol)\n",
    "        lasso.fit(X_train, y_train)\n",
    "        selected_features = np.sum(lasso.coef_ != 0)\n",
    "        \n",
    "        if selected_features == num_features:\n",
    "            break\n",
    "        elif selected_features < num_features:\n",
    "            alpha -= step\n",
    "        else:\n",
    "            alpha += step\n",
    "        step /= 2.0\n",
    "    \n",
    "    return alpha, lasso\n",
    "\n",
    "def lasso_ridge_debiasing(X_train, y_train, X_test, y_test, num_features, alpha_ridge=0):\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    \n",
    "    alpha_lasso, lasso = find_alpha_for_lasso(X_train, y_train, num_features)\n",
    "    \n",
    "    if np.sum(lasso.coef_ != 0) != num_features:\n",
    "        print(f\"Could not find alpha to select exactly {num_features} features. Selected {np.sum(lasso.coef_ != 0)} instead.\")\n",
    "    \n",
    "    selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "    if len(selected_features) == 0:\n",
    "        print(\"No features selected by Lasso. Try reducing num_features.\")\n",
    "        return None\n",
    "\n",
    "    # Ridge Regression on selected features\n",
    "    X_train_ridge = X_train[:, selected_features]\n",
    "    X_test_ridge = X_test[:, selected_features]\n",
    "    \n",
    "    ridge = Ridge(alpha=alpha_ridge)\n",
    "    ridge.fit(X_train_ridge, y_train)\n",
    "    ridge_coef = ridge.coef_\n",
    "\n",
    "    y_pred = ridge.predict(X_test_ridge)\n",
    "    pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "    \n",
    "    return pearson_corr\n",
    "\n",
    "\n",
    "results_combined = []\n",
    "\n",
    "for cnf in range(1,11):\n",
    "    \n",
    "    print(cnf)\n",
    "    X_test = XX.iloc[4000*(cnf-1):4000*cnf, :]\n",
    "    y_test = YY.iloc[4000*(cnf-1):4000*cnf]\n",
    "\n",
    "    X_train = XX.drop(XX.index[4000*(cnf-1):4000*cnf])\n",
    "    y_train = YY.drop(YY.index[4000*(cnf-1):4000*cnf])\n",
    "    \n",
    "    # Apply LassoRidge Debiasing algorithm for different numbers of selected features\n",
    "    num_features_list = np.arange(1,16)\n",
    "    results = []\n",
    "\n",
    "\n",
    "    for num_features in num_features_list:\n",
    "        result = lasso_ridge_debiasing(X_train, y_train, X_test, y_test, num_features, alpha_ridge=0)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "    \n",
    "    results_combined.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb424f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Debias_result = np.array(results_combined)\n",
    "np.save(\"Debias_result\", Debias_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0cc040",
   "metadata": {},
   "source": [
    "## Supervised Info. Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coff = []\n",
    "\n",
    "for cnf in range(1, 11):\n",
    "    print(f\"Processing configuration {cnf}\")\n",
    "    pear = []\n",
    "    arr = np.array(Imbalance_supervised[19+cnf,:15])\n",
    "    XXX = XX.iloc[:,arr]\n",
    "    \n",
    "    X_test = XXX.iloc[4000*(cnf-1):4000*cnf, :]\n",
    "    y_test = YY.iloc[4000*(cnf-1):4000*cnf]\n",
    "    \n",
    "    X_train = XXX.drop(XXX.index[4000*(cnf-1):4000*cnf])\n",
    "    y_train = YY.drop(YY.index[4000*(cnf-1):4000*cnf])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X_train)\n",
    "    X_te = scaler.transform(X_test)\n",
    "    \n",
    "    for m in range(1,16):\n",
    "        \n",
    "        \n",
    "        X_tr_sel = X_tr[:,:m]#.reshape(-1,1)\n",
    "        X_te_sel = X_te[:,:m]#.reshape(-1,1)\n",
    "        #print(X_tr_sel)\n",
    "\n",
    "        ridge = Ridge(alpha = 0)\n",
    "        ridge.fit(X_tr_sel,y_train)\n",
    "\n",
    "        y_pr = ridge.predict(X_te_sel)\n",
    "        pearson_corr_p, p_value = pearsonr( y_pr,y_test)\n",
    "        pear.append(abs(pearson_corr_p))\n",
    "        \n",
    "    pearson_coff.append(pear)\n",
    "    print(\"cnf:\" , cnf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "Info_imb_sup = np.array(pearson_coff) \n",
    "np.save(\"Info_imb_sup\", Info_imb_sup) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be87722",
   "metadata": {},
   "source": [
    "## Unsupervised Info. Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad71692",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coff = []\n",
    "\n",
    "for cnf in range(1, 11):\n",
    "    print(f\"Processing configuration {cnf}\")\n",
    "    pear = []\n",
    "    arr = np.array(imbalance_unsup[cnf-1,:])\n",
    "    XXX = XX.iloc[:,arr]\n",
    "    \n",
    "    # Split the data into training (4000) and testing (36000) sets\n",
    "    X_test = XXX.iloc[4000*(cnf-1):4000*cnf, :]\n",
    "    y_test = YY.iloc[4000*(cnf-1):4000*cnf]\n",
    "    \n",
    "    X_train = XXX.drop(XXX.index[4000*(cnf-1):4000*cnf])\n",
    "    y_train = YY.drop(YY.index[4000*(cnf-1):4000*cnf])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X_train)\n",
    "    X_te = scaler.transform(X_test)\n",
    "    \n",
    "    for m in range(1,16):\n",
    "        \n",
    "        \n",
    "        X_tr_sel = X_tr[:,:m]#.reshape(-1,1)\n",
    "        X_te_sel = X_te[:,:m]#.reshape(-1,1)\n",
    "        #print(X_tr_sel)\n",
    "\n",
    "        ridge = Ridge(alpha = 0)\n",
    "        ridge.fit(X_tr_sel,y_train)\n",
    "\n",
    "        y_pr = ridge.predict(X_te_sel)\n",
    "        pearson_corr_p, p_value = pearsonr( y_pr,y_test)\n",
    "        pear.append(abs(pearson_corr_p))\n",
    "        \n",
    "    pearson_coff.append(pear)\n",
    "    print(\"cnf:\" , cnf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Info_imb_unsup = np.array(pearson_coff)\n",
    "np.save(\"Info_imb_unsup\", Info_imb_unsup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf968a",
   "metadata": {},
   "source": [
    "## Random Selecton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d1c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coff = []\n",
    "\n",
    "for cnf in range(1, 11):\n",
    "    print(f\"Processing configuration {cnf}\")\n",
    "    pear = []\n",
    "    \n",
    "    # Repeating the experiment 1000 times\n",
    "    for _ in range(1000):\n",
    "        \n",
    "        arr = np.random.choice(60, size=60, replace=False)\n",
    "        XXX = XX.iloc[:, arr]\n",
    "        \n",
    "        X_test = XXX.iloc[4000*(cnf-1):4000*cnf, :]\n",
    "        y_test = YY.iloc[4000*(cnf-1):4000*cnf]\n",
    "        \n",
    "        X_train = XXX.drop(XXX.index[4000*(cnf-1):4000*cnf])\n",
    "        y_train = YY.drop(YY.index[4000*(cnf-1):4000*cnf])\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_tr = scaler.fit_transform(X_train)\n",
    "        X_te = scaler.transform(X_test)\n",
    "        \n",
    "        pear_temp = []\n",
    "        for m in range(1, 16):\n",
    "            X_tr_sel = X_tr[:, :m]\n",
    "            X_te_sel = X_te[:, :m]\n",
    "            \n",
    "            ridge = Ridge(alpha=0)\n",
    "            ridge.fit(X_tr_sel, y_train)\n",
    "            y_pr = ridge.predict(X_te_sel)\n",
    "            pearson_corr_p, _ = pearsonr(y_pr, y_test)\n",
    "            pear_temp.append(abs(pearson_corr_p))\n",
    "        \n",
    "        pear.append(pear_temp)\n",
    "    \n",
    "    avg_pear = np.mean(pear, axis=0)\n",
    "    pearson_coff.append(avg_pear)\n",
    "    print(f\"Configuration {cnf} completed\")\n",
    "\n",
    "print(\"Experiment completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_Results = np.array(pearson_coff)\n",
    "np.save(\"Random_Results\", Random_Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93360801",
   "metadata": {},
   "source": [
    "### Descripotrs vs propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f880b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Len = [0.5,1,1.5,2,2.5,3,3.5,4,4.5,5]\n",
    "leng = len(Len)\n",
    "Pearson_cnf = []\n",
    "\n",
    "for cnf in range(1,11):\n",
    "    Pearson = []\n",
    "    for i in range(0,60):\n",
    "        \n",
    "            pearson_corr_p, p_value = pearsonr( XX.iloc[4000*(cnf-1):4000*cnf ,i],YY.iloc[4000*(cnf-1):4000*cnf])\n",
    "            Pearson.append(pearson_corr_p)\n",
    "    Pearson_cnf.append(Pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aba22800",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y3 = np.array(Pearson_cnf)\n",
    "y3 = Y3.mean(axis=0)\n",
    "np.save(\"descripotr_vs_propensity\", Y3.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d4a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2d1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc61edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab2727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bca565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3456a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
